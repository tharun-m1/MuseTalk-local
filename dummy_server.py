import asyncio
import os
import traceback
import uuid
from aiohttp import web
import aiohttp_cors

from aiortc import RTCPeerConnection, RTCSessionDescription
import json
import time
import io
import cv2
import torch
import numpy as np
from pathlib import Path
from datetime import datetime
from pytz import timezone
from pydub import AudioSegment
import base64
from concurrent.futures import ThreadPoolExecutor
from transformers import WhisperModel

from musetalk.utils.blending import get_image_blending
from musetalk.utils.utils import load_all_model, datagen
from scripts.realtime_inference import Avatar
from musetalk.utils.audio_processor import AudioProcessor

# Global constants and state
AVATAR_ID = "pradeep"
BBOX_SHIFT = 12
BATCH_SIZE = 9
FPS = 12
MODEL_DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

audio_processor = vae = unet = pe = timesteps = avatar_model = whisper = None
pcs = set()

gpu_executor = ThreadPoolExecutor(max_workers=3)
generate_sema = asyncio.Semaphore(3)

# Job Queue
MAX_ACTIVE_JOBS = 2
job_queue = asyncio.Queue()
active_jobs = 0
total_requests_received = 0

# WebSocket connections
active_websockets = []

# Frame caching - ONLY addition to original
CACHE_DIR = "cached_frames"
MAX_CHUNKS_TO_CACHE = 20
current_chunk_count = 0
cached_chunks = {}  # {chunk_id: {"frames": [...], "audio": audio_data}}
cached_audio_data = {}  # {chunk_id: audio_bytes}
current_playback_index = 0  # For sequential playback

def extract_uuid_from_audio(audio_bytes):
    """Extract UUID from first 16 bytes of the audio data."""
    if len(audio_bytes) < 16:
        raise ValueError("Audio data too short to contain UUID")
    
    uuid_bytes = audio_bytes[:16]
    
    # Convert UUID bytes to string representation
    hex_digits = "0123456789abcdef"
    uuid_chars = []
    
    for i in range(16):
        uuid_chars.append(hex_digits[uuid_bytes[i] >> 4])
        uuid_chars.append(hex_digits[uuid_bytes[i] & 0x0f])
    
    # Format UUID with hyphens (8-4-4-4-12 format)
    uuid_str = ''.join(uuid_chars)
    uuid_str = f"{uuid_str[:8]}-{uuid_str[8:12]}-{uuid_str[12:16]}-{uuid_str[16:20]}-{uuid_str[20:]}"
    
    return uuid_str

def package_single_frame_for_webrtc(frame, uuid_str, chunk_id, is_final, status="success", error_message=""):
    """
    Package a single frame for WebRTC data channel transmission:
    - 16 bytes: UUID
    - 10 bytes: Status string (padded with null bytes)  
    - 100 bytes: Error message (padded with null bytes)
    - 4 bytes: Chunk ID
    - 1 byte: is_final boolean (1 = True, 0 = False)
    - 4 bytes: Frame data size
    - Frame data (variable length)
    """
    payload = bytearray()
    
    # 1. Add UUID (16 bytes)
    uuid_bytes = bytes.fromhex(uuid_str.replace('-', ''))
    payload.extend(uuid_bytes)
    
    # 2. Add status (10 bytes, padded with nulls)
    status_bytes = status.encode('utf-8')[:10].ljust(10, b'\0')
    payload.extend(status_bytes)
    
    # 3. Add error message (100 bytes, padded with nulls)
    error_bytes = error_message.encode('utf-8')[:100].ljust(100, b'\0')
    payload.extend(error_bytes)
    
    # 4. Add chunk_id (4 bytes)
    payload.extend(chunk_id.to_bytes(4, byteorder='big'))
    
    # 5. Add is_final boolean (1 byte)
    payload.extend((1 if is_final else 0).to_bytes(1, byteorder='big'))
    
    # 6. Process and add frame data
    # Resize frame to reduce data size
    target_size = (640, 360)  # (width, height) - adjust as needed
    resized_frame = cv2.resize(frame, target_size, interpolation=cv2.INTER_AREA)
    
    # Convert frame to JPEG
    _, buf = cv2.imencode('.jpg', resized_frame, [cv2.IMWRITE_JPEG_QUALITY, 85])
    frame_bytes = buf.tobytes()
    
    # Add frame size (4 bytes)
    payload.extend(len(frame_bytes).to_bytes(4, byteorder='big'))
    
    # Add frame data
    payload.extend(frame_bytes)
    
    return bytes(payload)

async def send_frame_via_webrtc_async(channel, frame, uuid_str, chunk_id, is_final, status="success", error_message=""):
    """Send a single frame via WebRTC data channel (async)"""
    try:
        if channel.readyState != "open":
            print(f"‚ö†Ô∏è Cannot send frame {chunk_id} - channel state: {channel.readyState}")
            return False
            
        # Package single frame for WebRTC
        response = package_single_frame_for_webrtc(frame, uuid_str, chunk_id, is_final, status, error_message)
        
        # Send via WebRTC data channel
        channel.send(response)
        
        print(f"‚úÖ Sent frame {chunk_id}/{uuid_str[:8]} (final: {is_final}) - {len(response)} bytes")
        return True
        
    except Exception as e:
        print(f"‚ùå Error sending frame {chunk_id}: {e}")
        return False

def generate_frames_streaming(audio_path):
    """
    Generate lip-sync frames and return them for streaming.
    Returns list of frames that can be sent individually.
    """
    global audio_processor, whisper
    t0 = time.time()
    print(f"[Timing] Start streaming processing audio chunk from: {audio_path}")

    whisper_feature, librosa_length = audio_processor.get_audio_feature(audio_path)
    t1 = time.time()
    print(f"[Timing] audio2feat took {t1 - t0:.3f} seconds")

    # Split features into chunks
    whisper_chunks = audio_processor.get_whisper_chunk(
        whisper_feature,
        MODEL_DEVICE,
        unet.model.dtype,
        whisper,  # pass the actual whisper model, not None
        librosa_length,
        fps=FPS,
        audio_padding_length_left=2,
        audio_padding_length_right=2,
    )
    total_frames = len(whisper_chunks)
    print(f"[Streaming] Will generate {total_frames} frames")
    t2 = time.time()
    print(f"[Timing] feature2chunks took {t2 - t1:.3f} seconds")

    # Get batch data for processing
    gen = datagen(whisper_chunks, avatar_model.input_latent_list_cycle, BATCH_SIZE)
    whisper_batch, latent_batch = next(gen)
    t3 = time.time()
    print(f"[Timing] datagen processing took {t3 - t2:.3f} seconds")

    # Remove torch.from_numpy, since whisper_batch is already a Tensor
    audio_feat = whisper_batch.to(MODEL_DEVICE, dtype=unet.model.dtype, non_blocking=True)
    audio_feat = pe(audio_feat)
    latent_batch = latent_batch.to(MODEL_DEVICE, dtype=unet.model.dtype, non_blocking=True)

    # Forward pass through unet (with autocast)
    with torch.cuda.amp.autocast():
        pred_latents = unet.model(latent_batch, timesteps, encoder_hidden_states=audio_feat).sample
    t4 = time.time()
    print(f"[Timing] Inference (unet forward pass) took {t4 - t3:.3f} seconds")

    # Decode with vae
    with torch.no_grad():
        recon = vae.decode_latents(pred_latents)
    t5 = time.time()
    print(f"[Timing] Decoding (vae) took {t5 - t4:.3f} seconds")

    # Process each frame and collect them
    full_frames = []
    for idx, patch in enumerate(recon):
        frame_start = time.time()
        
        # Process the patch
        np_patch = patch.cpu().numpy() if isinstance(patch, torch.Tensor) else patch
        if np_patch.ndim == 3 and np_patch.shape[0] == 3:
            np_patch = np_patch.transpose(1, 2, 0)
        np_patch = np_patch.astype(np.uint8)

        # Blend generated lip patch with original frame
        ori = avatar_model.frame_list_cycle[idx]
        x1, y1, x2, y2 = avatar_model.coord_list_cycle[idx]
        mask = avatar_model.mask_list_cycle[idx]
        mask_box = avatar_model.mask_coords_list_cycle[idx]
        resized_patch = cv2.resize(np_patch, (x2 - x1, y2 - y1), interpolation=cv2.INTER_LANCZOS4)
        blended = get_image_blending(ori, resized_patch, [x1, y1, x2, y2], mask, mask_box)
        
        # Add to frames list
        full_frames.append(blended)
        
        frame_end = time.time()
        print(f"[Timing] Processed frame {idx}/{total_frames} took {frame_end - frame_start:.3f} seconds")

    total_time = time.time() - t0
    print(f"[Timing] Total time for generating {len(full_frames)} frames: {total_time:.3f} seconds")
    
    return full_frames

# ONLY caching additions
def save_chunk_frames_and_audio(chunk_id, frames, audio_data):
    """Save frames and audio for a chunk to disk cache."""
    chunk_dir = os.path.join(CACHE_DIR, f"chunk_{chunk_id:03d}")
    os.makedirs(chunk_dir, exist_ok=True)
    
    # Save frames
    for idx, frame in enumerate(frames):
        frame_path = os.path.join(chunk_dir, f"frame_{idx:03d}.jpg")
        cv2.imwrite(frame_path, frame)
    
    # Save audio data
    audio_path = os.path.join(chunk_dir, "audio.wav")
    with open(audio_path, "wb") as f:
        f.write(audio_data)
    
    print(f"üíæ Cached {len(frames)} frames + audio for chunk {chunk_id}")

def load_chunk_frames_and_audio(chunk_id):
    """Load frames and audio for a chunk from disk cache."""
    chunk_dir = os.path.join(CACHE_DIR, f"chunk_{chunk_id:03d}")
    
    if not os.path.exists(chunk_dir):
        return None, None
    
    # Load frames
    frame_files = sorted([f for f in os.listdir(chunk_dir) if f.startswith("frame_") and f.endswith(".jpg")])
    frames = []
    
    for frame_file in frame_files:
        frame_path = os.path.join(chunk_dir, frame_file)
        frame = cv2.imread(frame_path)
        if frame is not None:
            frames.append(frame)
    
    # Load audio data
    audio_path = os.path.join(chunk_dir, "audio.wav")
    audio_data = None
    if os.path.exists(audio_path):
        with open(audio_path, "rb") as f:
            audio_data = f.read()
    
    return frames if frames else None, audio_data

async def stream_cached_chunk_response(channel, uuid_str):
    """Stream a cached chunk (frames + audio) in sequential order - this replaces voice AI response."""
    global cached_chunks, cached_audio_data, current_playback_index
    
    if len(cached_chunks) < MAX_CHUNKS_TO_CACHE:
        return False  # Not enough cached chunks yet
    
    # Use current playback index (sequential order: 0, 1, 2, ..., 19, 0, 1, 2, ...)
    chunk_id = current_playback_index
    frames = cached_chunks[chunk_id]
    audio_data = cached_audio_data.get(chunk_id)
    
    print(f"üöÄ Using cached chunk {chunk_id} as response ({len(frames)} frames) - sequential playback")
    
    # Stream the cached frames (same as original streaming)
    for idx, frame in enumerate(frames):
        is_final = (idx == len(frames) - 1)
        success = await send_frame_via_webrtc_async(channel, frame, uuid_str, idx, is_final)
        if not success:
            print(f"‚ö†Ô∏è Failed to send cached frame {idx}, continuing...")
        
        # Small delay to avoid overwhelming the channel
        if idx < len(frames) - 1:
            await asyncio.sleep(0.01)
    
    print(f"‚úÖ Successfully streamed {len(frames)} cached frames via WebRTC")
    
    # Send cached audio via WebSocket
    if active_websockets and audio_data:
        audio_b64 = base64.b64encode(audio_data).decode('utf-8')
        await send_audio_via_websocket(audio_b64, uuid_str)
        print(f"‚úÖ Sent cached audio for chunk {chunk_id} via WebSocket")
    
    # Move to next chunk in sequence (loop back to 0 after 19)
    current_playback_index = (current_playback_index + 1) % MAX_CHUNKS_TO_CACHE
    print(f"üìç Next playback will use chunk {current_playback_index}")
    
    return True

async def process_audio_and_respond(msg, channel):
    """
    Process audio data from WebRTC and generate lip-sync frames with streaming.
    """
    global active_jobs, current_chunk_count, cached_chunks
    try:
        # Get timestamp for logging
        ist = timezone('Asia/Kolkata')
        now = datetime.now(ist).strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]
        print(f"[{now}] Processing audio request with streaming...")

        # Extract audio bytes
        audio_bytes = bytes(msg)
        print(f"Received audio data size: {len(audio_bytes)} bytes")
        
        # Check if data contains UUID (should be at least 16 bytes)
        if len(audio_bytes) < 16:
            raise ValueError(f"Audio data too small: {len(audio_bytes)} bytes, expected at least 16 bytes for UUID")
        
        # Extract UUID (first 16 bytes) from audio data
        try:
            uuid_str = extract_uuid_from_audio(audio_bytes)
            print(f"Extracted UUID: {uuid_str}")
            # Remove UUID prefix from audio for processing
            audio_data = audio_bytes[16:]
        except Exception as e:
            print(f"Error extracting UUID: {e}")
            uuid_str = str(uuid.uuid4())  # Generate a new one if extraction fails
            audio_data = audio_bytes  # Use all data
        
        print(f"Audio data size after UUID extraction: {len(audio_data)} bytes")
        
        # FIRST CHECK: Do we have enough cached chunks? If yes, ignore incoming audio and use cached response
        if len(cached_chunks) >= MAX_CHUNKS_TO_CACHE:
            print(f"üöÄ Using cached response - ignoring incoming audio, no voice AI needed")
            await stream_cached_chunk_response(channel, uuid_str)
            return
        
        # Load cached chunks into memory if not already loaded (on server restart)
        if not cached_chunks and current_chunk_count == 0:
            print("üîÑ Loading existing cached chunks from disk...")
            for i in range(MAX_CHUNKS_TO_CACHE):
                existing_frames, existing_audio = load_chunk_frames_and_audio(i)
                if existing_frames and existing_audio:
                    cached_chunks[i] = existing_frames
                    cached_audio_data[i] = existing_audio
                    current_chunk_count = max(current_chunk_count, i + 1)
            
            if cached_chunks:
                print(f"‚úÖ Loaded {len(cached_chunks)} cached chunks from disk")
                # Check again if we have enough after loading
                if len(cached_chunks) >= MAX_CHUNKS_TO_CACHE:
                    await stream_cached_chunk_response(channel, uuid_str)
                    return
        
        # GENERATE NEW FRAMES (only for first MAX_CHUNKS_TO_CACHE requests)
        print(f"üî• Generating new chunk {current_chunk_count + 1}/{MAX_CHUNKS_TO_CACHE}")
        
        # Create buffer and convert to proper format for processing
        try:
            mp3_buf = io.BytesIO(audio_data)
            audio_seg = AudioSegment.from_file(mp3_buf, format="mp3", parameters=["-err_detect", "ignore_err"])
            
            # Get audio info for logging
            print(f"Audio properties: {audio_seg.channels} channels, {audio_seg.frame_rate}Hz, {audio_seg.duration_seconds:.2f}s")
            
            # Convert to WAV for processing
            wav_buf = io.BytesIO()
            audio_seg.export(wav_buf, format="wav", parameters=["-ar", "44100", "-ac", "1"])
            wav_bytes = wav_buf.getvalue()
        except Exception as audio_error:
            print(f"Error processing audio data: {audio_error}")
            raise ValueError(f"Failed to process audio data: {audio_error}")

        # Save to temporary file for processing
        tmp_dir = "tmp"
        os.makedirs(tmp_dir, exist_ok=True)
        audio_path = f"{tmp_dir}/audio_{time.time()}.wav"
        
        with open(audio_path, "wb") as f:
            f.write(wav_bytes)
        print(f"Saved audio to temporary file: {audio_path}")

        # Generate lip-sync frames (original logic)
        async with generate_sema:
            print(f"Starting streaming frame generation...")
            frames = await asyncio.get_running_loop().run_in_executor(
                gpu_executor, generate_frames_streaming, audio_path
            )
        
        if not frames:
            raise Exception("No frames available")
        
        print(f"‚úÖ Generated {len(frames)} frames, now streaming via WebRTC...")
        
        # Send each frame individually via WebRTC (original streaming logic)
        for idx, frame in enumerate(frames):
            is_final = (idx == len(frames) - 1)
            success = await send_frame_via_webrtc_async(channel, frame, uuid_str, idx, is_final)
            if not success:
                print(f"‚ö†Ô∏è Failed to send frame {idx}, continuing...")
            
            # Small delay to avoid overwhelming the channel
            if idx < len(frames) - 1:  # Don't delay after the last frame
                await asyncio.sleep(0.01)  # 10ms delay between frames
        
        print(f"‚úÖ Successfully streamed {len(frames)} frames via WebRTC")
        
        # Cache the generated frames AND audio for future use
        cached_chunks[current_chunk_count] = frames
        cached_audio_data[current_chunk_count] = wav_bytes  # Cache the processed audio
        await asyncio.get_running_loop().run_in_executor(
            gpu_executor, save_chunk_frames_and_audio, current_chunk_count, frames, wav_bytes
        )
        current_chunk_count += 1
        print(f"‚úÖ Generated and cached chunk {current_chunk_count}/{MAX_CHUNKS_TO_CACHE}")
        
        # Clean up temporary file
        try:
            os.remove(audio_path)
        except Exception as cleanup_error:
            print(f"Warning: Failed to remove temporary file {audio_path}: {cleanup_error}")

        # Send audio separately through WebSocket if needed (original logic)
        if active_websockets:
            # Convert audio to base64 for WebSocket transmission
            audio_b64 = base64.b64encode(audio_data).decode('utf-8')
            await send_audio_via_websocket(audio_b64, uuid_str)
    
    except Exception as e:
        print(f"‚ùå Error in streaming processing job: {e}")
        traceback.print_exc()
        
        # Send error response if possible
        if channel and channel.readyState == "open":
            try:
                # Send error as a single frame with chunk_id = 0
                error_frame = np.zeros((360, 640, 3), dtype=np.uint8)  # Empty frame
                error_uuid = uuid_str if 'uuid_str' in locals() else str(uuid.uuid4())
                
                await send_frame_via_webrtc_async(
                    channel, error_frame, error_uuid, 0, True, 
                    status="failed", error_message=str(e)[:99]
                )
                print(f"Sent error response through WebRTC")
            except Exception as send_err:
                print(f"Failed to send error response: {send_err}")
    
    finally:
        # Decrement active jobs counter and check queue
        active_jobs -= 1
        await maybe_start_next_job()

# Keep all other existing functions unchanged...
async def init_models(app):
    """Initialize ML models for lip-sync generation."""
    global audio_processor, vae, unet, pe, timesteps, avatar_model, whisper
    print("Loading models...")
    vae, unet, pe = load_all_model()
    audio_processor = AudioProcessor(feature_extractor_path="./models/whisper")
    pe = pe.half().to(MODEL_DEVICE)
    vae.vae = vae.vae.half().to(MODEL_DEVICE)
    unet.model = unet.model.half().to(MODEL_DEVICE)
    timesteps = torch.tensor([0], device=MODEL_DEVICE)
    whisper = WhisperModel.from_pretrained("./models/whisper")
    whisper = whisper.to(device=MODEL_DEVICE, dtype=unet.model.dtype).eval()
    whisper.requires_grad_(False)
    avatar_model = Avatar(
        avatar_id=AVATAR_ID,
        video_path="./data/video/dummy.mp4",
        bbox_shift=BBOX_SHIFT,
        batch_size=BATCH_SIZE,
        preparation=False,
        vae=vae
    )
    print("Models loaded successfully!")

async def offer(request):
    """Handle WebRTC offer from client."""
    try:
        params = await request.json()
        print(f"Received WebRTC offer from client")
        
        offer = RTCSessionDescription(sdp=params["sdp"], type=params["type"])
        pc = RTCPeerConnection()
        pcs.add(pc)

        @pc.on("datachannel")
        def on_datachannel(channel):
            if channel.label != "lipsyncdatachannel":
                print(f"Unexpected data channel: {channel.label}")
                return

            @channel.on("message")
            def handle(msg):
                print(f"Received WebRTC message of size {len(bytes(msg))} bytes")
                asyncio.create_task(queue_job(msg, channel))

        await pc.setRemoteDescription(offer)
        answer = await pc.createAnswer()
        await pc.setLocalDescription(answer)
        print("WebRTC connection established, sending answer to client")
        
        return web.Response(content_type="application/json", text=json.dumps({
            "sdp": pc.localDescription.sdp,
            "type": pc.localDescription.type
        }))
    except Exception as e:
        print(f"Error handling WebRTC offer: {e}")
        traceback.print_exc()
        return web.Response(status=500, text=f"Server error: {str(e)}")

async def queue_job(msg, channel):
    """Queue audio processing job."""
    global total_requests_received
    total_requests_received += 1
    print(f"üì• Request #{total_requests_received} received. Queue size: {job_queue.qsize()}")
    await job_queue.put((msg, channel))
    await maybe_start_next_job()

async def maybe_start_next_job():
    """Start processing the next job if resources are available."""
    global active_jobs
    if active_jobs >= MAX_ACTIVE_JOBS or job_queue.empty():
        return
    msg, channel = await job_queue.get()
    active_jobs += 1
    asyncio.create_task(process_audio_and_respond(msg, channel))

async def send_audio_via_websocket(audio_b64, uuid_str):
    """Send audio data via WebSocket with matching UUID."""
    ws_sent = False
    
    for ws in active_websockets:
        try:
            audio_msg = {
                "event": "playAudio",
                "media": {
                    "track": "inbound",
                    "payload": audio_b64
                },
                "streamId": uuid_str
            }
            
            await ws.send_json(audio_msg)
            ws_sent = True
            print(f"Sent audio with UUID {uuid_str} via WebSocket")
        except Exception as e:
            print(f"Error sending to WebSocket: {e}")
    
    if not ws_sent:
        print("‚ö†Ô∏è No active WebSockets to send audio")

async def websocket_handler(request):
    ws = web.WebSocketResponse()
    await ws.prepare(request)
    
    client_id = request.query.get('agent', 'unknown')
    print(f"New WebSocket connection from client: {client_id}")
    
    active_websockets.append(ws)
    
    try:
        async for msg in ws:
            if msg.type == web.WSMsgType.TEXT:
                try:
                    data = json.loads(msg.data)
                    print(f"Received WebSocket message: {data.get('event', 'unknown')}")
                    
                    if data.get('event') == 'media':
                        pass
                    elif data.get('event') == 'ping':
                        await ws.send_json({"event": "pong"})
                    
                except json.JSONDecodeError:
                    print(f"Received invalid JSON: {msg.data[:100]}...")
            
            elif msg.type == web.WSMsgType.ERROR:
                print(f"WebSocket error: {ws.exception()}")
    
    finally:
        if ws in active_websockets:
            active_websockets.remove(ws)
        print(f"WebSocket connection closed, client: {client_id}")
    
    return ws

async def ping(request):
    """Simple ping endpoint for connection testing."""
    return web.Response(text="pong")

def setup_application():
    """Configure and set up the web application."""
    app = web.Application()
    app.on_startup.append(init_models)
    
    # Configure CORS
    cors = aiohttp_cors.setup(app, defaults={
        "*": aiohttp_cors.ResourceOptions(
            allow_credentials=True,
            expose_headers="*",
            allow_headers="*",
            allow_methods=["POST", "GET", "OPTIONS", "DELETE", "PUT"]
        )
    })
    
    # Add routes
    app.router.add_route('GET', '/ws/', websocket_handler)
    
    # Add WebRTC and ping routes with CORS
    resource = cors.add(app.router.add_resource("/offer"))
    cors.add(resource.add_route("POST", offer))
    
    ping_resource = cors.add(app.router.add_resource("/ping"))
    cors.add(ping_resource.add_route("GET", ping))
    
    return app

def cleanup():
    """Clean up resources on shutdown."""
    # Close all peer connections
    for pc in pcs:
        pc.close()
    
    # Clean up temporary files
    for file in Path("tmp").glob("audio_*.wav"):
        try:
            file.unlink()
        except:
            pass

if __name__ == "__main__":
    try:
        app = setup_application()
        print(f"üöÄ Starting Dummy Server - will cache first {MAX_CHUNKS_TO_CACHE} chunks")
        web.run_app(app, host="0.0.0.0", port=8080)
    finally:
        cleanup()